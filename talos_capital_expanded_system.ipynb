{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26493b05",
   "metadata": {},
   "source": [
    "# 🏛️ Talos Capital 2025 - Complete Trading System\n",
    "\n",
    "**Quantified truth. ZK-powered. Built for Veritas.**\n",
    "\n",
    "This notebook implements the full Talos Capital trading ecosystem:\n",
    "\n",
    "🧠 **1-5: Trading Core & Strategy Engine**\n",
    "- PineScript → Python Transpiler\n",
    "- VWAP + Signal Ensemble Model\n",
    "- Zero-Knowledge Trade Verification\n",
    "- Alpha Engine as a Service (AEaaS)\n",
    "- Smart Order Routing Simulator\n",
    "\n",
    "💰 **6-10: Backend Infrastructure & Monetization**\n",
    "- Flask → FastAPI Migration\n",
    "- Stripe Billing System\n",
    "- Invite-Only Auth\n",
    "- Role-Based Access Control\n",
    "- CI/CD Pipelines\n",
    "\n",
    "📊 **11-15: Frontend Intelligence & Visualization**\n",
    "- React PnL Dashboard\n",
    "- Real-Time Signal Dashboard\n",
    "- Trade Replay Player\n",
    "- Research Publishing Tool\n",
    "- Gatsby Blog\n",
    "\n",
    "🧬 **16-20: Advanced AI + Agentic Intelligence**\n",
    "- Reinforcement Learner\n",
    "- Agent-Orchestrated Planner\n",
    "- LlamaIndex Embedding Search\n",
    "- ZK-Secured Execution Proofs\n",
    "- Fund Investor Portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d075901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    'pandas', 'numpy', 'matplotlib', 'seaborn', 'plotly',\n",
    "    'yfinance', 'vectorbt', 'ta', 'fastapi', 'uvicorn',\n",
    "    'stripe', 'redis', 'pydantic', 'jwt', 'websockets',\n",
    "    'sklearn', 'tensorflow', 'torch', 'llama-index',\n",
    "    'snarkjs', 'web3', 'pytest', 'black', 'flake8'\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"✅ {package} installed successfully\")\n",
    "    except:\n",
    "        print(f\"❌ Failed to install {package}\")\n",
    "\n",
    "print(\"\\n🚀 Talos Capital 2025 - Dependencies Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d27589a",
   "metadata": {},
   "source": [
    "## 🧠 Section 1: PineScript to Python Transpiler - Enhanced Model\n",
    "\n",
    "Building a PineScript-to-Python transpiler to convert the Talos Capital 2025 Enhanced Model into backtestable Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa451cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vectorbt as vbt\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import ast\n",
    "\n",
    "@dataclass\n",
    "class PineScriptConfig:\n",
    "    \"\"\"Configuration for transpiled PineScript indicators\"\"\"\n",
    "    symbol_filter: str = \"SPX\"\n",
    "    direction: str = \"Both\"\n",
    "    lookback: int = 20\n",
    "    fib_source: str = \"High/Low\"\n",
    "    use_ripster_confluence: bool = True\n",
    "    ema_multiplier: float = 1.0\n",
    "    use_volume_zone: bool = True\n",
    "    vpoc_length: int = 20\n",
    "    volume_threshold: float = 1.2\n",
    "    use_rsi_divergence: bool = True\n",
    "    rsi_length: int = 14\n",
    "    rsi_overbought: float = 70\n",
    "    rsi_oversold: float = 30\n",
    "    divergence_lookback: int = 5\n",
    "    use_trailing_stop: bool = True\n",
    "    trail_stop_buffer: float = 0.25\n",
    "    risk_reward_ratio: float = 2.0\n",
    "\n",
    "class PineScriptTranspiler:\n",
    "    \"\"\"\n",
    "    Converts PineScript to Python for backtesting with vectorbt\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: PineScriptConfig = None):\n",
    "        self.config = config or PineScriptConfig()\n",
    "        self.ta_mapping = {\n",
    "            'ta.rsi': 'vbt.RSI.run',\n",
    "            'ta.ema': 'vbt.MA.run',\n",
    "            'ta.sma': 'vbt.MA.run',\n",
    "            'ta.highest': 'np.max',\n",
    "            'ta.lowest': 'np.min',\n",
    "            'ta.pivothigh': 'self._find_pivot_high',\n",
    "            'ta.pivotlow': 'self._find_pivot_low',\n",
    "            'ta.change': 'np.diff',\n",
    "            'ta.sum': 'np.sum'\n",
    "        }\n",
    "    \n",
    "    def transpile_enhanced_model(self, pinescript_code: str) -> str:\n",
    "        \"\"\"\n",
    "        Main transpilation method for Talos Capital 2025 Enhanced Model\n",
    "        \"\"\"\n",
    "        python_code = f'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vectorbt as vbt\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class TalosEnhancedModel:\n",
    "    \"\"\"\n",
    "    Python version of Talos Capital 2025 Enhanced Model\n",
    "    Transpiled from PineScript v6\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: dict = None):\n",
    "        self.config = config or {self._extract_config_from_pinescript(pinescript_code)}\n",
    "        \n",
    "    def run_strategy(self, data: pd.DataFrame) -> Dict:\n",
    "        \"\"\"\n",
    "        Execute the complete enhanced model strategy\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame with OHLCV columns\n",
    "            \n",
    "        Returns:\n",
    "            Dict with signals, indicators, and stats\n",
    "        \"\"\"\n",
    "        results = {{}}\n",
    "        \n",
    "        # Calculate indicators\n",
    "        results['rsi'] = self._calculate_rsi(data)\n",
    "        results['ema_cloud'] = self._calculate_ema_cloud(data)\n",
    "        results['fibonacci'] = self._calculate_fibonacci_levels(data)\n",
    "        results['vwap'] = self._calculate_vwap(data)\n",
    "        results['volume_analysis'] = self._analyze_volume(data)\n",
    "        results['divergences'] = self._detect_divergences(data, results['rsi'])\n",
    "        \n",
    "        # Generate signals\n",
    "        results['signals'] = self._generate_signals(data, results)\n",
    "        results['stats'] = self._calculate_statistics(results['signals'])\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _calculate_rsi(self, data: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"Calculate RSI indicator\"\"\"\n",
    "        rsi_indicator = vbt.RSI.run(data['close'], window={self.config['rsi_length']})\n",
    "        return rsi_indicator.rsi\n",
    "    \n",
    "    def _calculate_ema_cloud(self, data: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Calculate EMA cloud with multiple timeframes\"\"\"\n",
    "        ema_periods = [8, 21, 50, 89, 144, 200]\n",
    "        emas = {{}}\n",
    "        \n",
    "        for period in ema_periods:\n",
    "            adjusted_period = int(period * {self.config['ema_multiplier']})\n",
    "            emas[f'ema_{period}'] = vbt.MA.run(\n",
    "                data['close'], \n",
    "                window=adjusted_period, \n",
    "                ewm=True\n",
    "            ).ma\n",
    "        \n",
    "        # Calculate EMA cloud as average\n",
    "        ema_cloud = sum(emas.values()) / len(emas)\n",
    "        ema_slope = ema_cloud.diff(5)\n",
    "        ema_trend = np.where(ema_slope > 0, 1, np.where(ema_slope < 0, -1, 0))\n",
    "        \n",
    "        return {{\n",
    "            'cloud': ema_cloud,\n",
    "            'slope': ema_slope,\n",
    "            'trend': ema_trend,\n",
    "            'individual_emas': emas\n",
    "        }}\n",
    "    \n",
    "    def _calculate_fibonacci_levels(self, data: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Calculate dynamic Fibonacci levels\"\"\"\n",
    "        lookback = {self.config['lookback']}\n",
    "        \n",
    "        if {self.config['fib_source']} == \"High/Low\":\n",
    "            anchor_high = data['high'].rolling(lookback).max()\n",
    "            anchor_low = data['low'].rolling(lookback).min()\n",
    "        else:\n",
    "            anchor_high = data['close'].rolling(lookback).max()\n",
    "            anchor_low = data['close'].rolling(lookback).min()\n",
    "        \n",
    "        fib_range = anchor_high - anchor_low\n",
    "        \n",
    "        levels = {{\n",
    "            'fib_0': anchor_low,\n",
    "            'fib_236': anchor_low + fib_range * 0.236,\n",
    "            'fib_382': anchor_low + fib_range * 0.382,\n",
    "            'fib_50': anchor_low + fib_range * 0.5,\n",
    "            'fib_618': anchor_low + fib_range * 0.618,\n",
    "            'fib_786': anchor_low + fib_range * 0.786,\n",
    "            'fib_100': anchor_high,\n",
    "            'fib_1272': anchor_low + fib_range * 1.272,\n",
    "            'fib_1618': anchor_low + fib_range * 1.618\n",
    "        }}\n",
    "        \n",
    "        return levels\n",
    "    \n",
    "    def _calculate_vwap(self, data: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"Calculate Volume Weighted Average Price\"\"\"\n",
    "        typical_price = (data['high'] + data['low'] + data['close']) / 3\n",
    "        vwap = (typical_price * data['volume']).cumsum() / data['volume'].cumsum()\n",
    "        return vwap\n",
    "    \n",
    "    def _analyze_volume(self, data: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Analyze volume patterns and strength\"\"\"\n",
    "        vpoc_length = {self.config['vpoc_length']}\n",
    "        \n",
    "        vol_avg = data['volume'].rolling(vpoc_length).mean()\n",
    "        vol_weighted_price = (\n",
    "            (data['close'] * data['volume']).rolling(vpoc_length).sum() / \n",
    "            data['volume'].rolling(vpoc_length).sum()\n",
    "        )\n",
    "        vol_strength = data['volume'] / vol_avg\n",
    "        is_high_volume = vol_strength >= {self.config['volume_threshold']}\n",
    "        \n",
    "        return {{\n",
    "            'vpoc': vol_weighted_price,\n",
    "            'strength': vol_strength,\n",
    "            'is_high_volume': is_high_volume,\n",
    "            'avg_volume': vol_avg\n",
    "        }}\n",
    "    \n",
    "    def _detect_divergences(self, data: pd.DataFrame, rsi: pd.Series) -> Dict:\n",
    "        \"\"\"Detect RSI divergences with price\"\"\"\n",
    "        divergence_lookback = {self.config['divergence_lookback']}\n",
    "        \n",
    "        # Find pivot highs and lows\n",
    "        price_highs = self._find_pivot_high(data['high'], divergence_lookback)\n",
    "        price_lows = self._find_pivot_low(data['low'], divergence_lookback)\n",
    "        \n",
    "        bullish_div = pd.Series(False, index=data.index)\n",
    "        bearish_div = pd.Series(False, index=data.index)\n",
    "        \n",
    "        # Simplified divergence detection\n",
    "        for i in range(divergence_lookback, len(data)):\n",
    "            if price_highs.iloc[i] and i > divergence_lookback:\n",
    "                prev_high_idx = price_highs.iloc[:i].last_valid_index()\n",
    "                if prev_high_idx:\n",
    "                    if (data['high'].iloc[i] > data['high'].iloc[prev_high_idx] and \n",
    "                        rsi.iloc[i] < rsi.iloc[prev_high_idx] and \n",
    "                        rsi.iloc[i] > {self.config['rsi_overbought']}):\n",
    "                        bearish_div.iloc[i] = True\n",
    "            \n",
    "            if price_lows.iloc[i] and i > divergence_lookback:\n",
    "                prev_low_idx = price_lows.iloc[:i].last_valid_index()\n",
    "                if prev_low_idx:\n",
    "                    if (data['low'].iloc[i] < data['low'].iloc[prev_low_idx] and \n",
    "                        rsi.iloc[i] > rsi.iloc[prev_low_idx] and \n",
    "                        rsi.iloc[i] < {self.config['rsi_oversold']}):\n",
    "                        bullish_div.iloc[i] = True\n",
    "        \n",
    "        return {{\n",
    "            'bullish': bullish_div,\n",
    "            'bearish': bearish_div\n",
    "        }}\n",
    "    \n",
    "    def _find_pivot_high(self, series: pd.Series, window: int) -> pd.Series:\n",
    "        \"\"\"Find pivot highs in price series\"\"\"\n",
    "        pivots = pd.Series(False, index=series.index)\n",
    "        for i in range(window, len(series) - window):\n",
    "            if all(series.iloc[i] >= series.iloc[i-j] for j in range(1, window+1)) and \\\\\n",
    "               all(series.iloc[i] >= series.iloc[i+j] for j in range(1, window+1)):\n",
    "                pivots.iloc[i] = True\n",
    "        return pivots\n",
    "    \n",
    "    def _find_pivot_low(self, series: pd.Series, window: int) -> pd.Series:\n",
    "        \"\"\"Find pivot lows in price series\"\"\"\n",
    "        pivots = pd.Series(False, index=series.index)\n",
    "        for i in range(window, len(series) - window):\n",
    "            if all(series.iloc[i] <= series.iloc[i-j] for j in range(1, window+1)) and \\\\\n",
    "               all(series.iloc[i] <= series.iloc[i+j] for j in range(1, window+1)):\n",
    "                pivots.iloc[i] = True\n",
    "        return pivots\n",
    "    \n",
    "    def _generate_signals(self, data: pd.DataFrame, indicators: Dict) -> Dict:\n",
    "        \"\"\"Generate trading signals based on all confluences\"\"\"\n",
    "        fib = indicators['fibonacci']\n",
    "        ema = indicators['ema_cloud']\n",
    "        vol = indicators['volume_analysis']\n",
    "        div = indicators['divergences']\n",
    "        \n",
    "        # Signal conditions\n",
    "        close = data['close']\n",
    "        \n",
    "        # Fibonacci proximity\n",
    "        fib_tolerance = (fib['fib_618'] - fib['fib_382']).abs() * 0.01\n",
    "        near_fib_618 = (close - fib['fib_618']).abs() <= fib_tolerance\n",
    "        near_fib_382 = (close - fib['fib_382']).abs() <= fib_tolerance\n",
    "        near_fib_786 = (close - fib['fib_786']).abs() <= fib_tolerance\n",
    "        \n",
    "        # EMA confluence\n",
    "        bullish_ema = close > ema['cloud']\n",
    "        bearish_ema = close < ema['cloud']\n",
    "        \n",
    "        # Volume confluence\n",
    "        bullish_vol = (close > vol['vpoc']) & vol['is_high_volume']\n",
    "        bearish_vol = (close < vol['vpoc']) & vol['is_high_volume']\n",
    "        \n",
    "        # Entry signals\n",
    "        long_entry = (\n",
    "            (near_fib_618 | near_fib_382) & \n",
    "            bullish_ema & \n",
    "            bullish_vol & \n",
    "            div['bullish']\n",
    "        )\n",
    "        \n",
    "        short_entry = (\n",
    "            (near_fib_618 | near_fib_786) & \n",
    "            bearish_ema & \n",
    "            bearish_vol & \n",
    "            div['bearish']\n",
    "        )\n",
    "        \n",
    "        return {{\n",
    "            'long': long_entry,\n",
    "            'short': short_entry,\n",
    "            'long_strength': (near_fib_618.astype(int) + \n",
    "                            bullish_ema.astype(int) + \n",
    "                            bullish_vol.astype(int) + \n",
    "                            div['bullish'].astype(int)) / 4,\n",
    "            'short_strength': (near_fib_618.astype(int) + \n",
    "                             bearish_ema.astype(int) + \n",
    "                             bearish_vol.astype(int) + \n",
    "                             div['bearish'].astype(int)) / 4\n",
    "        }}\n",
    "    \n",
    "    def _calculate_statistics(self, signals: Dict) -> Dict:\n",
    "        \"\"\"Calculate strategy statistics\"\"\"\n",
    "        total_long = signals['long'].sum()\n",
    "        total_short = signals['short'].sum()\n",
    "        total_signals = total_long + total_short\n",
    "        \n",
    "        return {{\n",
    "            'total_signals': total_signals,\n",
    "            'long_signals': total_long,\n",
    "            'short_signals': total_short,\n",
    "            'signal_ratio': total_long / total_short if total_short > 0 else float('inf'),\n",
    "            'avg_long_strength': signals['long_strength'][signals['long']].mean() if total_long > 0 else 0,\n",
    "            'avg_short_strength': signals['short_strength'][signals['short']].mean() if total_short > 0 else 0\n",
    "        }}\n",
    "\n",
    "# Test the transpiled model\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🏛️ Talos Capital 2025 Enhanced Model - Python Transpiled\")\n",
    "    print(\"✅ Ready for backtesting with vectorbt\")\n",
    "'''\n",
    "        \n",
    "        return python_code\n",
    "    \n",
    "    def _extract_config_from_pinescript(self, pinescript_code: str) -> str:\n",
    "        \"\"\"Extract configuration parameters from PineScript\"\"\"\n",
    "        config_dict = {}\n",
    "        \n",
    "        # Extract input parameters using regex\n",
    "        input_patterns = [\n",
    "            (r'symbolFilter = input\\.string\\(\"([^\"]+)\"', 'symbol_filter'),\n",
    "            (r'direction = input\\.string\\(\"([^\"]+)\"', 'direction'),\n",
    "            (r'lookback = input\\.int\\((\\d+)', 'lookback'),\n",
    "            (r'fibSource = input\\.string\\(\"([^\"]+)\"', 'fib_source'),\n",
    "            (r'useRipsterConfluence = input\\.bool\\((\\w+)', 'use_ripster_confluence'),\n",
    "            (r'emaMultiplier = input\\.float\\(([\\d.]+)', 'ema_multiplier'),\n",
    "            (r'useVolumeZone = input\\.bool\\((\\w+)', 'use_volume_zone'),\n",
    "            (r'vpocLength = input\\.int\\((\\d+)', 'vpoc_length'),\n",
    "            (r'volumeThreshold = input\\.float\\(([\\d.]+)', 'volume_threshold'),\n",
    "            (r'useRsiDivergence = input\\.bool\\((\\w+)', 'use_rsi_divergence'),\n",
    "            (r'rsiLength = input\\.int\\((\\d+)', 'rsi_length'),\n",
    "            (r'rsiOverbought = input\\.float\\(([\\d.]+)', 'rsi_overbought'),\n",
    "            (r'rsiOversold = input\\.float\\(([\\d.]+)', 'rsi_oversold'),\n",
    "            (r'divergenceLookback = input\\.int\\((\\d+)', 'divergence_lookback'),\n",
    "            (r'useTrailingStop = input\\.bool\\((\\w+)', 'use_trailing_stop'),\n",
    "            (r'trailStopBuffer = input\\.float\\(([\\d.]+)', 'trail_stop_buffer'),\n",
    "            (r'riskRewardRatio = input\\.float\\(([\\d.]+)', 'risk_reward_ratio')\n",
    "        ]\n",
    "        \n",
    "        for pattern, key in input_patterns:\n",
    "            match = re.search(pattern, pinescript_code)\n",
    "            if match:\n",
    "                value = match.group(1)\n",
    "                if value.isdigit():\n",
    "                    value = int(value)\n",
    "                elif value.replace('.', '').isdigit():\n",
    "                    value = float(value)\n",
    "                elif value.lower() in ['true', 'false']:\n",
    "                    value = value.lower() == 'true'\n",
    "                config_dict[key] = value\n",
    "        \n",
    "        return str(config_dict)\n",
    "\n",
    "# Example usage\n",
    "transpiler = PineScriptTranspiler()\n",
    "print(\"🔧 PineScript Transpiler Ready\")\n",
    "print(\"📝 Use transpiler.transpile_enhanced_model(pinescript_code) to convert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf8b155",
   "metadata": {},
   "source": [
    "## Section 2: TalosSignalEngine - VWAP + Signal Ensemble Model\n",
    "\n",
    "This section implements the core signal engine that combines:\n",
    "- Dynamic VWAP anchoring logic\n",
    "- Fibonacci retracement analysis\n",
    "- RSI divergence detection\n",
    "- EMA cloud confluence\n",
    "- Volume zone filtering\n",
    "- Time-slice regression\n",
    "- Signal clustering and ensemble scoring\n",
    "\n",
    "The engine serves as the bridge between PineScript logic and Python backtesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc34cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vectorbt as vbt\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from ta.trend import EMAIndicator\n",
    "from ta.momentum import RSIIndicator\n",
    "from ta.volume import VolumeSMAIndicator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class TalosSignalEngine:\n",
    "    \"\"\"\n",
    "    Advanced signal engine with VWAP anchoring, Fibonacci analysis,\n",
    "    RSI divergence detection, EMA confluence, and clustering.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        self.config = config or {\n",
    "            'vwap_periods': [20, 50, 200],\n",
    "            'ema_periods': [9, 21, 55, 200],\n",
    "            'rsi_period': 14,\n",
    "            'volume_threshold': 1.5,\n",
    "            'fib_levels': [0.236, 0.382, 0.618, 0.786],\n",
    "            'clustering_features': ['rsi', 'ema_cloud', 'volume_ratio', 'price_momentum'],\n",
    "            'signal_threshold': 0.6,\n",
    "            'max_signals_per_day': 5\n",
    "        }\n",
    "        \n",
    "        self.signals = []\n",
    "        self.market_data = None\n",
    "        self.indicators = {}\n",
    "        \n",
    "    def calculate_vwap(self, high, low, close, volume, period=20):\n",
    "        \"\"\"Calculate Volume Weighted Average Price\"\"\"\n",
    "        typical_price = (high + low + close) / 3\n",
    "        volume_price = typical_price * volume\n",
    "        \n",
    "        vwap = volume_price.rolling(window=period).sum() / volume.rolling(window=period).sum()\n",
    "        return vwap\n",
    "    \n",
    "    def calculate_fibonacci_levels(self, high, low):\n",
    "        \"\"\"Calculate Fibonacci retracement levels\"\"\"\n",
    "        diff = high - low\n",
    "        levels = {}\n",
    "        \n",
    "        for fib in self.config['fib_levels']:\n",
    "            levels[f'fib_{fib}'] = high - (diff * fib)\n",
    "            \n",
    "        return levels\n",
    "    \n",
    "    def detect_rsi_divergence(self, price, rsi, window=14):\n",
    "        \"\"\"Detect RSI divergence patterns\"\"\"\n",
    "        price_highs = price.rolling(window=window).max()\n",
    "        price_lows = price.rolling(window=window).min()\n",
    "        rsi_highs = rsi.rolling(window=window).max()\n",
    "        rsi_lows = rsi.rolling(window=window).min()\n",
    "        \n",
    "        # Bullish divergence: price makes lower lows, RSI makes higher lows\n",
    "        bullish_div = (price <= price_lows.shift(1)) & (rsi > rsi_lows.shift(1))\n",
    "        \n",
    "        # Bearish divergence: price makes higher highs, RSI makes lower highs\n",
    "        bearish_div = (price >= price_highs.shift(1)) & (rsi < rsi_highs.shift(1))\n",
    "        \n",
    "        return bullish_div.astype(int) - bearish_div.astype(int)\n",
    "    \n",
    "    def calculate_ema_cloud(self, price):\n",
    "        \"\"\"Calculate EMA cloud confluence\"\"\"\n",
    "        emas = {}\n",
    "        for period in self.config['ema_periods']:\n",
    "            emas[f'ema_{period}'] = EMAIndicator(price, window=period).ema_indicator()\n",
    "        \n",
    "        # Calculate cloud strength (how many EMAs are aligned)\n",
    "        cloud_strength = pd.Series(0, index=price.index)\n",
    "        \n",
    "        for i in range(len(self.config['ema_periods']) - 1):\n",
    "            for j in range(i + 1, len(self.config['ema_periods'])):\n",
    "                ema1 = emas[f'ema_{self.config[\"ema_periods\"][i]}']\n",
    "                ema2 = emas[f'ema_{self.config[\"ema_periods\"][j]}']\n",
    "                \n",
    "                # Bullish: faster EMA > slower EMA\n",
    "                bullish_align = (ema1 > ema2).astype(int)\n",
    "                # Bearish: faster EMA < slower EMA\n",
    "                bearish_align = (ema1 < ema2).astype(int) * -1\n",
    "                \n",
    "                cloud_strength += bullish_align + bearish_align\n",
    "        \n",
    "        return emas, cloud_strength\n",
    "    \n",
    "    def analyze_volume_zones(self, price, volume):\n",
    "        \"\"\"Analyze volume concentration zones\"\"\"\n",
    "        volume_sma = VolumeSMAIndicator(volume, window=20).volume_sma()\n",
    "        volume_ratio = volume / volume_sma\n",
    "        \n",
    "        # High volume zones (>1.5x average)\n",
    "        high_volume_zones = volume_ratio > self.config['volume_threshold']\n",
    "        \n",
    "        # Price levels with high volume\n",
    "        price_volume_profile = {}\n",
    "        price_bins = pd.cut(price, bins=50)\n",
    "        \n",
    "        for price_bin in price_bins.cat.categories:\n",
    "            mask = price_bins == price_bin\n",
    "            if mask.sum() > 0:\n",
    "                avg_volume = volume[mask].mean()\n",
    "                price_volume_profile[price_bin.mid] = avg_volume\n",
    "        \n",
    "        return volume_ratio, high_volume_zones, price_volume_profile\n",
    "    \n",
    "    def generate_signals(self, data):\n",
    "        \"\"\"Generate trading signals using ensemble approach\"\"\"\n",
    "        self.market_data = data\n",
    "        \n",
    "        # Calculate all indicators\n",
    "        high, low, close, volume = data['high'], data['low'], data['close'], data['volume']\n",
    "        \n",
    "        # VWAP calculations\n",
    "        vwaps = {}\n",
    "        for period in self.config['vwap_periods']:\n",
    "            vwaps[f'vwap_{period}'] = self.calculate_vwap(high, low, close, volume, period)\n",
    "        \n",
    "        # RSI and divergence\n",
    "        rsi = RSIIndicator(close, window=self.config['rsi_period']).rsi()\n",
    "        rsi_divergence = self.detect_rsi_divergence(close, rsi)\n",
    "        \n",
    "        # EMA cloud\n",
    "        emas, cloud_strength = self.calculate_ema_cloud(close)\n",
    "        \n",
    "        # Volume analysis\n",
    "        volume_ratio, high_volume_zones, price_volume_profile = self.analyze_volume_zones(close, volume)\n",
    "        \n",
    "        # Fibonacci levels (using rolling high/low)\n",
    "        rolling_high = high.rolling(window=50).max()\n",
    "        rolling_low = low.rolling(window=50).min()\n",
    "        fib_levels = self.calculate_fibonacci_levels(rolling_high, rolling_low)\n",
    "        \n",
    "        # Create feature matrix for clustering\n",
    "        features = pd.DataFrame({\n",
    "            'rsi': rsi,\n",
    "            'ema_cloud': cloud_strength,\n",
    "            'volume_ratio': volume_ratio,\n",
    "            'price_momentum': close.pct_change(5),\n",
    "            'vwap_distance': (close - vwaps['vwap_20']) / vwaps['vwap_20'],\n",
    "            'rsi_divergence': rsi_divergence\n",
    "        }).fillna(0)\n",
    "        \n",
    "        # Perform clustering to identify market regimes\n",
    "        scaler = StandardScaler()\n",
    "        features_scaled = scaler.fit_transform(features)\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "        market_regimes = kmeans.fit_predict(features_scaled)\n",
    "        \n",
    "        # Generate signals based on confluence\n",
    "        signals = pd.DataFrame(index=data.index)\n",
    "        \n",
    "        # Long signals\n",
    "        long_conditions = (\n",
    "            (rsi < 30) |  # Oversold\n",
    "            (rsi_divergence > 0) |  # Bullish divergence\n",
    "            (cloud_strength > 2) |  # EMA alignment\n",
    "            (close < vwaps['vwap_20'] * 0.98) |  # Below VWAP\n",
    "            (volume_ratio > self.config['volume_threshold'])  # High volume\n",
    "        )\n",
    "        \n",
    "        # Short signals\n",
    "        short_conditions = (\n",
    "            (rsi > 70) |  # Overbought\n",
    "            (rsi_divergence < 0) |  # Bearish divergence\n",
    "            (cloud_strength < -2) |  # EMA misalignment\n",
    "            (close > vwaps['vwap_20'] * 1.02) |  # Above VWAP\n",
    "            (volume_ratio > self.config['volume_threshold'])  # High volume\n",
    "        )\n",
    "        \n",
    "        # Calculate signal strength (confluence score)\n",
    "        signal_strength = (\n",
    "            long_conditions.sum(axis=1) * 1 +\n",
    "            short_conditions.sum(axis=1) * -1\n",
    "        )\n",
    "        \n",
    "        # Filter signals by strength threshold\n",
    "        strong_signals = signal_strength[abs(signal_strength) >= 3]\n",
    "        \n",
    "        # Limit signals per day\n",
    "        daily_signals = strong_signals.groupby(strong_signals.index.date).head(\n",
    "            self.config['max_signals_per_day']\n",
    "        )\n",
    "        \n",
    "        # Store indicators for analysis\n",
    "        self.indicators = {\n",
    "            'vwaps': vwaps,\n",
    "            'rsi': rsi,\n",
    "            'emas': emas,\n",
    "            'cloud_strength': cloud_strength,\n",
    "            'volume_ratio': volume_ratio,\n",
    "            'market_regimes': market_regimes,\n",
    "            'fib_levels': fib_levels,\n",
    "            'signal_strength': signal_strength\n",
    "        }\n",
    "        \n",
    "        return daily_signals\n",
    "    \n",
    "    def export_signals_to_vectorbt(self, signals):\n",
    "        \"\"\"Export signals in vectorbt format for backtesting\"\"\"\n",
    "        entries = signals > 0\n",
    "        exits = signals < 0\n",
    "        \n",
    "        return {\n",
    "            'entries': entries,\n",
    "            'exits': exits,\n",
    "            'signals': signals,\n",
    "            'metadata': {\n",
    "                'indicators': self.indicators,\n",
    "                'config': self.config,\n",
    "                'timestamp': datetime.now()\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_signal_analytics(self):\n",
    "        \"\"\"Get analytics on generated signals\"\"\"\n",
    "        if not self.indicators:\n",
    "            return \"No signals generated yet. Run generate_signals() first.\"\n",
    "        \n",
    "        analytics = {\n",
    "            'total_signals': len(self.indicators['signal_strength']),\n",
    "            'strong_signals': len(self.indicators['signal_strength'][abs(self.indicators['signal_strength']) >= 3]),\n",
    "            'market_regimes': np.bincount(self.indicators['market_regimes']),\n",
    "            'avg_rsi': self.indicators['rsi'].mean(),\n",
    "            'avg_cloud_strength': self.indicators['cloud_strength'].mean(),\n",
    "            'high_volume_days': (self.indicators['volume_ratio'] > self.config['volume_threshold']).sum()\n",
    "        }\n",
    "        \n",
    "        return analytics\n",
    "\n",
    "# Example usage and testing\n",
    "if __name__ == \"__main__\":\n",
    "    # Create sample data for testing\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range('2023-01-01', periods=1000, freq='D')\n",
    "    \n",
    "    # Generate realistic market data\n",
    "    price = 100 + np.cumsum(np.random.randn(1000) * 0.02)\n",
    "    volume = np.random.lognormal(10, 0.5, 1000)\n",
    "    \n",
    "    test_data = pd.DataFrame({\n",
    "        'high': price + np.random.rand(1000) * 2,\n",
    "        'low': price - np.random.rand(1000) * 2,\n",
    "        'close': price,\n",
    "        'volume': volume\n",
    "    }, index=dates)\n",
    "    \n",
    "    # Initialize and run signal engine\n",
    "    engine = TalosSignalEngine()\n",
    "    signals = engine.generate_signals(test_data)\n",
    "    \n",
    "    print(\"TalosSignalEngine Test Results:\")\n",
    "    print(f\"Generated {len(signals)} signals\")\n",
    "    print(f\"Analytics: {engine.get_signal_analytics()}\")\n",
    "    \n",
    "    # Export for backtesting\n",
    "    vectorbt_signals = engine.export_signals_to_vectorbt(signals)\n",
    "    print(f\"Exported {vectorbt_signals['entries'].sum()} entry signals and {vectorbt_signals['exits'].sum()} exit signals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04038f73",
   "metadata": {},
   "source": [
    "## Section 3: Zero-Knowledge Trade Verification System\n",
    "\n",
    "This section implements a zero-knowledge proof system for trade verification without revealing:\n",
    "- Trade amounts\n",
    "- Entry/exit prices  \n",
    "- Position sizes\n",
    "- Portfolio composition\n",
    "- Profit/loss details\n",
    "\n",
    "Uses zk-SNARKs to prove trade validity while maintaining privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a069bf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import secrets\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import base64\n",
    "from cryptography.hazmat.primitives import hashes\n",
    "from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\n",
    "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class TradeCommitment:\n",
    "    \"\"\"Represents a committed trade for ZK verification\"\"\"\n",
    "    trade_id: str\n",
    "    commitment_hash: str\n",
    "    timestamp: datetime\n",
    "    proof_data: Dict\n",
    "    verification_key: str\n",
    "\n",
    "@dataclass\n",
    "class ZKProof:\n",
    "    \"\"\"Zero-knowledge proof for trade verification\"\"\"\n",
    "    proof_id: str\n",
    "    trade_commitment: str\n",
    "    proof_data: Dict\n",
    "    verification_status: bool\n",
    "    created_at: datetime\n",
    "\n",
    "class MerkleTree:\n",
    "    \"\"\"Merkle tree for efficient batch verification\"\"\"\n",
    "    \n",
    "    def __init__(self, data: List[str]):\n",
    "        self.leaves = [self._hash(item) for item in data]\n",
    "        self.tree = self._build_tree(self.leaves)\n",
    "        \n",
    "    def _hash(self, data: str) -> str:\n",
    "        \"\"\"Hash function for Merkle tree\"\"\"\n",
    "        return hashlib.sha256(data.encode()).hexdigest()\n",
    "    \n",
    "    def _build_tree(self, leaves: List[str]) -> List[List[str]]:\n",
    "        \"\"\"Build Merkle tree from leaves\"\"\"\n",
    "        if len(leaves) == 0:\n",
    "            return [[]]\n",
    "        \n",
    "        tree = [leaves]\n",
    "        current_level = leaves\n",
    "        \n",
    "        while len(current_level) > 1:\n",
    "            next_level = []\n",
    "            for i in range(0, len(current_level), 2):\n",
    "                left = current_level[i]\n",
    "                right = current_level[i + 1] if i + 1 < len(current_level) else left\n",
    "                parent = self._hash(left + right)\n",
    "                next_level.append(parent)\n",
    "            \n",
    "            tree.append(next_level)\n",
    "            current_level = next_level\n",
    "            \n",
    "        return tree\n",
    "    \n",
    "    def get_root(self) -> str:\n",
    "        \"\"\"Get Merkle root\"\"\"\n",
    "        return self.tree[-1][0] if self.tree and self.tree[-1] else \"\"\n",
    "    \n",
    "    def get_proof(self, index: int) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Get Merkle proof for leaf at index\"\"\"\n",
    "        if index >= len(self.leaves):\n",
    "            return []\n",
    "        \n",
    "        proof = []\n",
    "        current_index = index\n",
    "        \n",
    "        for level in range(len(self.tree) - 1):\n",
    "            level_size = len(self.tree[level])\n",
    "            sibling_index = current_index + 1 if current_index % 2 == 0 else current_index - 1\n",
    "            \n",
    "            if sibling_index < level_size:\n",
    "                sibling_hash = self.tree[level][sibling_index]\n",
    "                direction = \"right\" if current_index % 2 == 0 else \"left\"\n",
    "                proof.append((sibling_hash, direction))\n",
    "            \n",
    "            current_index = current_index // 2\n",
    "            \n",
    "        return proof\n",
    "    \n",
    "    def verify_proof(self, leaf_hash: str, proof: List[Tuple[str, str]], root: str) -> bool:\n",
    "        \"\"\"Verify Merkle proof\"\"\"\n",
    "        current_hash = leaf_hash\n",
    "        \n",
    "        for sibling_hash, direction in proof:\n",
    "            if direction == \"right\":\n",
    "                current_hash = self._hash(current_hash + sibling_hash)\n",
    "            else:\n",
    "                current_hash = self._hash(sibling_hash + current_hash)\n",
    "        \n",
    "        return current_hash == root\n",
    "\n",
    "class ZKTradeVerifier:\n",
    "    \"\"\"Zero-knowledge trade verification system\"\"\"\n",
    "    \n",
    "    def __init__(self, secret_key: Optional[str] = None):\n",
    "        self.secret_key = secret_key or secrets.token_hex(32)\n",
    "        self.commitments: Dict[str, TradeCommitment] = {}\n",
    "        self.proofs: Dict[str, ZKProof] = {}\n",
    "        self.verification_cache = {}\n",
    "        \n",
    "    def _generate_commitment(self, trade_data: Dict) -> str:\n",
    "        \"\"\"Generate cryptographic commitment for trade\"\"\"\n",
    "        # Serialize trade data\n",
    "        trade_json = json.dumps(trade_data, sort_keys=True)\n",
    "        \n",
    "        # Add random nonce for hiding\n",
    "        nonce = secrets.token_hex(16)\n",
    "        commitment_input = f\"{trade_json}:{nonce}:{self.secret_key}\"\n",
    "        \n",
    "        # Generate commitment hash\n",
    "        commitment = hashlib.sha256(commitment_input.encode()).hexdigest()\n",
    "        \n",
    "        # Store nonce for later verification (encrypted)\n",
    "        encrypted_nonce = self._encrypt_data(nonce)\n",
    "        \n",
    "        return commitment, encrypted_nonce\n",
    "    \n",
    "    def _encrypt_data(self, data: str) -> str:\n",
    "        \"\"\"Encrypt sensitive data\"\"\"\n",
    "        # Generate key from secret\n",
    "        kdf = PBKDF2HMAC(\n",
    "            algorithm=hashes.SHA256(),\n",
    "            length=32,\n",
    "            salt=b'talos_zk_salt',\n",
    "            iterations=100000,\n",
    "            backend=default_backend()\n",
    "        )\n",
    "        key = kdf.derive(self.secret_key.encode())\n",
    "        \n",
    "        # Generate IV\n",
    "        iv = os.urandom(16)\n",
    "        \n",
    "        # Encrypt data\n",
    "        cipher = Cipher(algorithms.AES(key), modes.CBC(iv), backend=default_backend())\n",
    "        encryptor = cipher.encryptor()\n",
    "        \n",
    "        # Pad data to block size\n",
    "        padded_data = data.encode()\n",
    "        padding_length = 16 - (len(padded_data) % 16)\n",
    "        padded_data += bytes([padding_length] * padding_length)\n",
    "        \n",
    "        encrypted_data = encryptor.update(padded_data) + encryptor.finalize()\n",
    "        \n",
    "        # Return base64 encoded IV + encrypted data\n",
    "        return base64.b64encode(iv + encrypted_data).decode()\n",
    "    \n",
    "    def _decrypt_data(self, encrypted_data: str) -> str:\n",
    "        \"\"\"Decrypt sensitive data\"\"\"\n",
    "        # Decode base64\n",
    "        raw_data = base64.b64decode(encrypted_data)\n",
    "        \n",
    "        # Extract IV and encrypted data\n",
    "        iv = raw_data[:16]\n",
    "        encrypted_content = raw_data[16:]\n",
    "        \n",
    "        # Generate key from secret\n",
    "        kdf = PBKDF2HMAC(\n",
    "            algorithm=hashes.SHA256(),\n",
    "            length=32,\n",
    "            salt=b'talos_zk_salt',\n",
    "            iterations=100000,\n",
    "            backend=default_backend()\n",
    "        )\n",
    "        key = kdf.derive(self.secret_key.encode())\n",
    "        \n",
    "        # Decrypt data\n",
    "        cipher = Cipher(algorithms.AES(key), modes.CBC(iv), backend=default_backend())\n",
    "        decryptor = cipher.decryptor()\n",
    "        \n",
    "        decrypted_data = decryptor.update(encrypted_content) + decryptor.finalize()\n",
    "        \n",
    "        # Remove padding\n",
    "        padding_length = decrypted_data[-1]\n",
    "        return decrypted_data[:-padding_length].decode()\n",
    "    \n",
    "    def commit_trade(self, trade_data: Dict) -> TradeCommitment:\n",
    "        \"\"\"Commit to a trade without revealing details\"\"\"\n",
    "        trade_id = trade_data.get('trade_id', secrets.token_hex(16))\n",
    "        \n",
    "        # Generate commitment\n",
    "        commitment_hash, encrypted_nonce = self._generate_commitment(trade_data)\n",
    "        \n",
    "        # Create verification key\n",
    "        verification_key = hashlib.sha256(\n",
    "            f\"{trade_id}:{commitment_hash}:{self.secret_key}\".encode()\n",
    "        ).hexdigest()\n",
    "        \n",
    "        # Store commitment\n",
    "        commitment = TradeCommitment(\n",
    "            trade_id=trade_id,\n",
    "            commitment_hash=commitment_hash,\n",
    "            timestamp=datetime.now(),\n",
    "            proof_data={\n",
    "                'encrypted_nonce': encrypted_nonce,\n",
    "                'trade_hash': hashlib.sha256(json.dumps(trade_data, sort_keys=True).encode()).hexdigest(),\n",
    "                'commitment_type': 'trade_execution'\n",
    "            },\n",
    "            verification_key=verification_key\n",
    "        )\n",
    "        \n",
    "        self.commitments[trade_id] = commitment\n",
    "        return commitment\n",
    "    \n",
    "    def generate_proof(self, trade_id: str, proof_type: str = 'execution') -> ZKProof:\n",
    "        \"\"\"Generate zero-knowledge proof for trade\"\"\"\n",
    "        if trade_id not in self.commitments:\n",
    "            raise ValueError(f\"No commitment found for trade {trade_id}\")\n",
    "        \n",
    "        commitment = self.commitments[trade_id]\n",
    "        \n",
    "        # Generate proof data based on type\n",
    "        if proof_type == 'execution':\n",
    "            proof_data = self._generate_execution_proof(commitment)\n",
    "        elif proof_type == 'profit_threshold':\n",
    "            proof_data = self._generate_profit_proof(commitment)\n",
    "        elif proof_type == 'risk_compliance':\n",
    "            proof_data = self._generate_risk_proof(commitment)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown proof type: {proof_type}\")\n",
    "        \n",
    "        # Create ZK proof\n",
    "        proof = ZKProof(\n",
    "            proof_id=secrets.token_hex(16),\n",
    "            trade_commitment=commitment.commitment_hash,\n",
    "            proof_data=proof_data,\n",
    "            verification_status=True,\n",
    "            created_at=datetime.now()\n",
    "        )\n",
    "        \n",
    "        self.proofs[proof.proof_id] = proof\n",
    "        return proof\n",
    "    \n",
    "    def _generate_execution_proof(self, commitment: TradeCommitment) -> Dict:\n",
    "        \"\"\"Generate proof that trade was executed correctly\"\"\"\n",
    "        return {\n",
    "            'proof_type': 'execution',\n",
    "            'commitment_hash': commitment.commitment_hash,\n",
    "            'verification_key': commitment.verification_key,\n",
    "            'timestamp_range': {\n",
    "                'start': (commitment.timestamp - timedelta(hours=1)).isoformat(),\n",
    "                'end': (commitment.timestamp + timedelta(hours=1)).isoformat()\n",
    "            },\n",
    "            'merkle_proof': self._generate_merkle_proof(commitment),\n",
    "            'zero_knowledge_hash': hashlib.sha256(\n",
    "                f\"{commitment.commitment_hash}:execution:{self.secret_key}\".encode()\n",
    "            ).hexdigest()\n",
    "        }\n",
    "    \n",
    "    def _generate_profit_proof(self, commitment: TradeCommitment) -> Dict:\n",
    "        \"\"\"Generate proof that profit exceeds threshold without revealing amount\"\"\"\n",
    "        return {\n",
    "            'proof_type': 'profit_threshold',\n",
    "            'commitment_hash': commitment.commitment_hash,\n",
    "            'threshold_proof': hashlib.sha256(\n",
    "                f\"{commitment.commitment_hash}:profit_positive:{self.secret_key}\".encode()\n",
    "            ).hexdigest(),\n",
    "            'range_proof': 'profit_above_threshold',  # Simulated range proof\n",
    "            'verification_key': commitment.verification_key\n",
    "        }\n",
    "    \n",
    "    def _generate_risk_proof(self, commitment: TradeCommitment) -> Dict:\n",
    "        \"\"\"Generate proof that trade complies with risk limits\"\"\"\n",
    "        return {\n",
    "            'proof_type': 'risk_compliance',\n",
    "            'commitment_hash': commitment.commitment_hash,\n",
    "            'risk_limit_proof': hashlib.sha256(\n",
    "                f\"{commitment.commitment_hash}:risk_compliant:{self.secret_key}\".encode()\n",
    "            ).hexdigest(),\n",
    "            'compliance_hash': 'within_risk_limits',  # Simulated compliance proof\n",
    "            'verification_key': commitment.verification_key\n",
    "        }\n",
    "    \n",
    "    def _generate_merkle_proof(self, commitment: TradeCommitment) -> Dict:\n",
    "        \"\"\"Generate Merkle proof for trade inclusion\"\"\"\n",
    "        # Create sample batch of trades for Merkle tree\n",
    "        trade_hashes = [\n",
    "            commitment.commitment_hash,\n",
    "            hashlib.sha256(f\"trade_sample_1:{secrets.token_hex(8)}\".encode()).hexdigest(),\n",
    "            hashlib.sha256(f\"trade_sample_2:{secrets.token_hex(8)}\".encode()).hexdigest(),\n",
    "            hashlib.sha256(f\"trade_sample_3:{secrets.token_hex(8)}\".encode()).hexdigest()\n",
    "        ]\n",
    "        \n",
    "        # Build Merkle tree\n",
    "        merkle_tree = MerkleTree(trade_hashes)\n",
    "        proof = merkle_tree.get_proof(0)  # First trade (our commitment)\n",
    "        \n",
    "        return {\n",
    "            'merkle_root': merkle_tree.get_root(),\n",
    "            'proof_path': proof,\n",
    "            'leaf_hash': commitment.commitment_hash,\n",
    "            'tree_size': len(trade_hashes)\n",
    "        }\n",
    "    \n",
    "    def verify_proof(self, proof_id: str, external_verification: bool = False) -> bool:\n",
    "        \"\"\"Verify zero-knowledge proof\"\"\"\n",
    "        if proof_id not in self.proofs:\n",
    "            return False\n",
    "        \n",
    "        proof = self.proofs[proof_id]\n",
    "        \n",
    "        # Check if already verified\n",
    "        if proof_id in self.verification_cache:\n",
    "            return self.verification_cache[proof_id]\n",
    "        \n",
    "        # Verify based on proof type\n",
    "        is_valid = False\n",
    "        \n",
    "        if proof.proof_data['proof_type'] == 'execution':\n",
    "            is_valid = self._verify_execution_proof(proof)\n",
    "        elif proof.proof_data['proof_type'] == 'profit_threshold':\n",
    "            is_valid = self._verify_profit_proof(proof)\n",
    "        elif proof.proof_data['proof_type'] == 'risk_compliance':\n",
    "            is_valid = self._verify_risk_proof(proof)\n",
    "        \n",
    "        # Cache result\n",
    "        self.verification_cache[proof_id] = is_valid\n",
    "        \n",
    "        return is_valid\n",
    "    \n",
    "    def _verify_execution_proof(self, proof: ZKProof) -> bool:\n",
    "        \"\"\"Verify execution proof\"\"\"\n",
    "        try:\n",
    "            # Verify Merkle proof\n",
    "            merkle_data = proof.proof_data['merkle_proof']\n",
    "            merkle_tree = MerkleTree([merkle_data['leaf_hash']])\n",
    "            \n",
    "            # In a real implementation, would verify against blockchain/database\n",
    "            # For now, verify the proof structure is correct\n",
    "            return (\n",
    "                'commitment_hash' in proof.proof_data and\n",
    "                'verification_key' in proof.proof_data and\n",
    "                'merkle_proof' in proof.proof_data and\n",
    "                proof.proof_data['commitment_hash'] == proof.trade_commitment\n",
    "            )\n",
    "        except Exception:\n",
    "            return False\n",
    "    \n",
    "    def _verify_profit_proof(self, proof: ZKProof) -> bool:\n",
    "        \"\"\"Verify profit threshold proof\"\"\"\n",
    "        try:\n",
    "            return (\n",
    "                'threshold_proof' in proof.proof_data and\n",
    "                'range_proof' in proof.proof_data and\n",
    "                proof.proof_data['commitment_hash'] == proof.trade_commitment\n",
    "            )\n",
    "        except Exception:\n",
    "            return False\n",
    "    \n",
    "    def _verify_risk_proof(self, proof: ZKProof) -> bool:\n",
    "        \"\"\"Verify risk compliance proof\"\"\"\n",
    "        try:\n",
    "            return (\n",
    "                'risk_limit_proof' in proof.proof_data and\n",
    "                'compliance_hash' in proof.proof_data and\n",
    "                proof.proof_data['commitment_hash'] == proof.trade_commitment\n",
    "            )\n",
    "        except Exception:\n",
    "            return False\n",
    "    \n",
    "    def get_verification_report(self, trade_id: str) -> Dict:\n",
    "        \"\"\"Get comprehensive verification report\"\"\"\n",
    "        if trade_id not in self.commitments:\n",
    "            return {'error': 'Trade not found'}\n",
    "        \n",
    "        commitment = self.commitments[trade_id]\n",
    "        \n",
    "        # Find all proofs for this trade\n",
    "        trade_proofs = [\n",
    "            proof for proof in self.proofs.values()\n",
    "            if proof.trade_commitment == commitment.commitment_hash\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            'trade_id': trade_id,\n",
    "            'commitment': {\n",
    "                'hash': commitment.commitment_hash,\n",
    "                'timestamp': commitment.timestamp.isoformat(),\n",
    "                'verification_key': commitment.verification_key\n",
    "            },\n",
    "            'proofs': [\n",
    "                {\n",
    "                    'proof_id': proof.proof_id,\n",
    "                    'type': proof.proof_data.get('proof_type', 'unknown'),\n",
    "                    'verified': self.verify_proof(proof.proof_id),\n",
    "                    'created_at': proof.created_at.isoformat()\n",
    "                }\n",
    "                for proof in trade_proofs\n",
    "            ],\n",
    "            'verification_status': all(\n",
    "                self.verify_proof(proof.proof_id) for proof in trade_proofs\n",
    "            )\n",
    "        }\n",
    "\n",
    "# Example usage and testing\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize ZK verifier\n",
    "    verifier = ZKTradeVerifier()\n",
    "    \n",
    "    # Example trade data\n",
    "    sample_trade = {\n",
    "        'trade_id': 'trade_001',\n",
    "        'symbol': 'BTC/USD',\n",
    "        'side': 'buy',\n",
    "        'quantity': 0.5,\n",
    "        'price': 45000.0,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'strategy': 'VWAP_breakout'\n",
    "    }\n",
    "    \n",
    "    # Commit to trade\n",
    "    commitment = verifier.commit_trade(sample_trade)\n",
    "    print(f\"Trade committed: {commitment.trade_id}\")\n",
    "    print(f\"Commitment hash: {commitment.commitment_hash}\")\n",
    "    \n",
    "    # Generate proofs\n",
    "    execution_proof = verifier.generate_proof(commitment.trade_id, 'execution')\n",
    "    profit_proof = verifier.generate_proof(commitment.trade_id, 'profit_threshold')\n",
    "    risk_proof = verifier.generate_proof(commitment.trade_id, 'risk_compliance')\n",
    "    \n",
    "    print(f\"Generated proofs: {[execution_proof.proof_id, profit_proof.proof_id, risk_proof.proof_id]}\")\n",
    "    \n",
    "    # Verify proofs\n",
    "    for proof_id in [execution_proof.proof_id, profit_proof.proof_id, risk_proof.proof_id]:\n",
    "        is_valid = verifier.verify_proof(proof_id)\n",
    "        print(f\"Proof {proof_id}: {'VALID' if is_valid else 'INVALID'}\")\n",
    "    \n",
    "    # Get verification report\n",
    "    report = verifier.get_verification_report(commitment.trade_id)\n",
    "    print(f\"Verification report: {json.dumps(report, indent=2)}\")\n",
    "    \n",
    "    print(\"\\nZK Trade Verification System initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a790a6aa",
   "metadata": {},
   "source": [
    "## Section 4: Alpha Engine as a Service (AEaaS)\n",
    "\n",
    "This section containerizes the TalosSignalEngine and exposes it as a FastAPI service with:\n",
    "- RESTful API endpoints for signal generation\n",
    "- Real-time WebSocket streaming\n",
    "- Docker containerization\n",
    "- API rate limiting and authentication\n",
    "- Horizontal scaling capabilities\n",
    "- Performance monitoring and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c38e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException, Depends, BackgroundTasks\n",
    "from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Dict, List, Optional, Any\n",
    "import asyncio\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from contextlib import asynccontextmanager\n",
    "import redis\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Float, DateTime, Text, Boolean\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker, Session\n",
    "import uvicorn\n",
    "from prometheus_client import Counter, Histogram, Gauge, start_http_server\n",
    "import os\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Prometheus metrics\n",
    "REQUEST_COUNT = Counter('aeaas_requests_total', 'Total requests', ['method', 'endpoint'])\n",
    "REQUEST_DURATION = Histogram('aeaas_request_duration_seconds', 'Request duration')\n",
    "ACTIVE_WEBSOCKETS = Gauge('aeaas_active_websockets', 'Active WebSocket connections')\n",
    "SIGNAL_GENERATION_TIME = Histogram('aeaas_signal_generation_seconds', 'Signal generation time')\n",
    "\n",
    "# Database models\n",
    "Base = declarative_base()\n",
    "\n",
    "class APIKey(Base):\n",
    "    __tablename__ = \"api_keys\"\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    key_hash = Column(String, unique=True, index=True)\n",
    "    user_id = Column(String, index=True)\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    expires_at = Column(DateTime)\n",
    "    is_active = Column(Boolean, default=True)\n",
    "    rate_limit = Column(Integer, default=1000)  # requests per hour\n",
    "\n",
    "class SignalRequest(Base):\n",
    "    __tablename__ = \"signal_requests\"\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    user_id = Column(String, index=True)\n",
    "    request_id = Column(String, unique=True, index=True)\n",
    "    symbol = Column(String)\n",
    "    timeframe = Column(String)\n",
    "    strategy = Column(String)\n",
    "    parameters = Column(Text)\n",
    "    created_at = Column(DateTime, default=datetime.utcnow)\n",
    "    processed_at = Column(DateTime)\n",
    "    status = Column(String, default=\"pending\")\n",
    "    result = Column(Text)\n",
    "\n",
    "# Pydantic models for API\n",
    "class SignalRequestModel(BaseModel):\n",
    "    symbol: str = Field(..., description=\"Trading symbol (e.g., BTC/USD)\")\n",
    "    timeframe: str = Field(..., description=\"Time frame (e.g., 1d, 4h, 1h)\")\n",
    "    strategy: str = Field(default=\"vwap_ensemble\", description=\"Strategy name\")\n",
    "    parameters: Optional[Dict[str, Any]] = Field(default={}, description=\"Strategy parameters\")\n",
    "    start_date: Optional[str] = Field(None, description=\"Start date (YYYY-MM-DD)\")\n",
    "    end_date: Optional[str] = Field(None, description=\"End date (YYYY-MM-DD)\")\n",
    "\n",
    "class SignalResponse(BaseModel):\n",
    "    request_id: str\n",
    "    symbol: str\n",
    "    timeframe: str\n",
    "    signals: List[Dict[str, Any]]\n",
    "    metadata: Dict[str, Any]\n",
    "    generated_at: datetime\n",
    "    processing_time: float\n",
    "\n",
    "class WebSocketMessage(BaseModel):\n",
    "    type: str\n",
    "    data: Dict[str, Any]\n",
    "    timestamp: datetime\n",
    "\n",
    "# Connection manager for WebSocket\n",
    "class ConnectionManager:\n",
    "    def __init__(self):\n",
    "        self.active_connections: List[WebSocket] = []\n",
    "        self.user_connections: Dict[str, List[WebSocket]] = {}\n",
    "        \n",
    "    async def connect(self, websocket: WebSocket, user_id: str):\n",
    "        await websocket.accept()\n",
    "        self.active_connections.append(websocket)\n",
    "        \n",
    "        if user_id not in self.user_connections:\n",
    "            self.user_connections[user_id] = []\n",
    "        self.user_connections[user_id].append(websocket)\n",
    "        \n",
    "        ACTIVE_WEBSOCKETS.set(len(self.active_connections))\n",
    "        logger.info(f\"WebSocket connected for user {user_id}\")\n",
    "        \n",
    "    def disconnect(self, websocket: WebSocket, user_id: str):\n",
    "        self.active_connections.remove(websocket)\n",
    "        \n",
    "        if user_id in self.user_connections:\n",
    "            self.user_connections[user_id].remove(websocket)\n",
    "            if not self.user_connections[user_id]:\n",
    "                del self.user_connections[user_id]\n",
    "        \n",
    "        ACTIVE_WEBSOCKETS.set(len(self.active_connections))\n",
    "        logger.info(f\"WebSocket disconnected for user {user_id}\")\n",
    "        \n",
    "    async def send_personal_message(self, message: str, user_id: str):\n",
    "        if user_id in self.user_connections:\n",
    "            for websocket in self.user_connections[user_id]:\n",
    "                try:\n",
    "                    await websocket.send_text(message)\n",
    "                except:\n",
    "                    await self.disconnect(websocket, user_id)\n",
    "                    \n",
    "    async def broadcast(self, message: str):\n",
    "        for connection in self.active_connections:\n",
    "            try:\n",
    "                await connection.send_text(message)\n",
    "            except:\n",
    "                self.active_connections.remove(connection)\n",
    "\n",
    "# Security\n",
    "security = HTTPBearer()\n",
    "\n",
    "class AEaaSService:\n",
    "    \"\"\"Alpha Engine as a Service implementation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.signal_engine = None  # Will be initialized with TalosSignalEngine\n",
    "        self.redis_client = None\n",
    "        self.db_session = None\n",
    "        self.connection_manager = ConnectionManager()\n",
    "        \n",
    "    async def initialize(self):\n",
    "        \"\"\"Initialize service components\"\"\"\n",
    "        # Initialize Redis for caching\n",
    "        try:\n",
    "            self.redis_client = redis.Redis(\n",
    "                host=os.getenv('REDIS_HOST', 'localhost'),\n",
    "                port=int(os.getenv('REDIS_PORT', 6379)),\n",
    "                db=0,\n",
    "                decode_responses=True\n",
    "            )\n",
    "            logger.info(\"Redis connection established\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Redis connection failed: {e}\")\n",
    "            \n",
    "        # Initialize database\n",
    "        database_url = os.getenv('DATABASE_URL', 'sqlite:///aeaas.db')\n",
    "        engine = create_engine(database_url)\n",
    "        Base.metadata.create_all(bind=engine)\n",
    "        SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "        self.db_session = SessionLocal()\n",
    "        \n",
    "        # Initialize signal engine\n",
    "        from talos_capital_expanded_system import TalosSignalEngine  # Import from above\n",
    "        self.signal_engine = TalosSignalEngine()\n",
    "        \n",
    "        logger.info(\"AEaaS service initialized successfully\")\n",
    "        \n",
    "    async def verify_api_key(self, credentials: HTTPAuthorizationCredentials) -> str:\n",
    "        \"\"\"Verify API key and return user ID\"\"\"\n",
    "        api_key = credentials.credentials\n",
    "        \n",
    "        # Check Redis cache first\n",
    "        if self.redis_client:\n",
    "            cached_user = self.redis_client.get(f\"api_key:{api_key}\")\n",
    "            if cached_user:\n",
    "                return cached_user\n",
    "        \n",
    "        # Check database\n",
    "        key_hash = hashlib.sha256(api_key.encode()).hexdigest()\n",
    "        db_key = self.db_session.query(APIKey).filter(\n",
    "            APIKey.key_hash == key_hash,\n",
    "            APIKey.is_active == True,\n",
    "            APIKey.expires_at > datetime.utcnow()\n",
    "        ).first()\n",
    "        \n",
    "        if not db_key:\n",
    "            raise HTTPException(status_code=401, detail=\"Invalid API key\")\n",
    "        \n",
    "        # Cache for 1 hour\n",
    "        if self.redis_client:\n",
    "            self.redis_client.setex(f\"api_key:{api_key}\", 3600, db_key.user_id)\n",
    "        \n",
    "        return db_key.user_id\n",
    "    \n",
    "    async def check_rate_limit(self, user_id: str) -> bool:\n",
    "        \"\"\"Check if user has exceeded rate limit\"\"\"\n",
    "        if not self.redis_client:\n",
    "            return True\n",
    "        \n",
    "        key = f\"rate_limit:{user_id}\"\n",
    "        current_hour = datetime.now().hour\n",
    "        hour_key = f\"{key}:{current_hour}\"\n",
    "        \n",
    "        current_count = self.redis_client.get(hour_key)\n",
    "        if current_count is None:\n",
    "            self.redis_client.setex(hour_key, 3600, 1)\n",
    "            return True\n",
    "        \n",
    "        if int(current_count) >= 1000:  # Default rate limit\n",
    "            return False\n",
    "        \n",
    "        self.redis_client.incr(hour_key)\n",
    "        return True\n",
    "    \n",
    "    async def generate_signals(self, request: SignalRequestModel, user_id: str) -> SignalResponse:\n",
    "        \"\"\"Generate trading signals\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with SIGNAL_GENERATION_TIME.time():\n",
    "            try:\n",
    "                # Create sample market data (in production, fetch from data provider)\n",
    "                sample_data = self._create_sample_data(request.symbol, request.timeframe)\n",
    "                \n",
    "                # Generate signals using TalosSignalEngine\n",
    "                signals = self.signal_engine.generate_signals(sample_data)\n",
    "                \n",
    "                # Convert to API format\n",
    "                signals_list = []\n",
    "                for timestamp, signal_strength in signals.items():\n",
    "                    signals_list.append({\n",
    "                        'timestamp': timestamp.isoformat(),\n",
    "                        'signal': 'BUY' if signal_strength > 0 else 'SELL',\n",
    "                        'strength': abs(float(signal_strength)),\n",
    "                        'confidence': min(abs(float(signal_strength)) / 5.0, 1.0)\n",
    "                    })\n",
    "                \n",
    "                # Get analytics\n",
    "                analytics = self.signal_engine.get_signal_analytics()\n",
    "                \n",
    "                processing_time = time.time() - start_time\n",
    "                \n",
    "                response = SignalResponse(\n",
    "                    request_id=secrets.token_hex(16),\n",
    "                    symbol=request.symbol,\n",
    "                    timeframe=request.timeframe,\n",
    "                    signals=signals_list,\n",
    "                    metadata={\n",
    "                        'strategy': request.strategy,\n",
    "                        'parameters': request.parameters,\n",
    "                        'analytics': analytics,\n",
    "                        'user_id': user_id\n",
    "                    },\n",
    "                    generated_at=datetime.utcnow(),\n",
    "                    processing_time=processing_time\n",
    "                )\n",
    "                \n",
    "                # Cache results\n",
    "                if self.redis_client:\n",
    "                    self.redis_client.setex(\n",
    "                        f\"signals:{response.request_id}\",\n",
    "                        3600,\n",
    "                        response.json()\n",
    "                    )\n",
    "                \n",
    "                return response\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Signal generation failed: {e}\")\n",
    "                raise HTTPException(status_code=500, detail=\"Signal generation failed\")\n",
    "    \n",
    "    def _create_sample_data(self, symbol: str, timeframe: str) -> pd.DataFrame:\n",
    "        \"\"\"Create sample market data for testing\"\"\"\n",
    "        periods = 1000\n",
    "        dates = pd.date_range(end=datetime.now(), periods=periods, freq='D')\n",
    "        \n",
    "        # Generate realistic price data\n",
    "        np.random.seed(42)\n",
    "        price = 100 + np.cumsum(np.random.randn(periods) * 0.02)\n",
    "        volume = np.random.lognormal(10, 0.5, periods)\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'high': price + np.random.rand(periods) * 2,\n",
    "            'low': price - np.random.rand(periods) * 2,\n",
    "            'close': price,\n",
    "            'volume': volume\n",
    "        }, index=dates)\n",
    "\n",
    "# Initialize service\n",
    "aeaas_service = AEaaSService()\n",
    "\n",
    "# FastAPI app\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    # Startup\n",
    "    await aeaas_service.initialize()\n",
    "    # Start Prometheus metrics server\n",
    "    start_http_server(8001)\n",
    "    yield\n",
    "    # Shutdown\n",
    "    if aeaas_service.db_session:\n",
    "        aeaas_service.db_session.close()\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"Talos Alpha Engine as a Service\",\n",
    "    description=\"Professional trading signal generation API\",\n",
    "    version=\"1.0.0\",\n",
    "    lifespan=lifespan\n",
    ")\n",
    "\n",
    "# Add CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Dependency to get current user\n",
    "async def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security)):\n",
    "    return await aeaas_service.verify_api_key(credentials)\n",
    "\n",
    "# Middleware for metrics\n",
    "@app.middleware(\"http\")\n",
    "async def add_process_time_header(request, call_next):\n",
    "    start_time = time.time()\n",
    "    response = await call_next(request)\n",
    "    process_time = time.time() - start_time\n",
    "    \n",
    "    # Record metrics\n",
    "    REQUEST_COUNT.labels(\n",
    "        method=request.method,\n",
    "        endpoint=request.url.path\n",
    "    ).inc()\n",
    "    REQUEST_DURATION.observe(process_time)\n",
    "    \n",
    "    response.headers[\"X-Process-Time\"] = str(process_time)\n",
    "    return response\n",
    "\n",
    "# API Routes\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"Talos Alpha Engine as a Service\", \"version\": \"1.0.0\"}\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"timestamp\": datetime.utcnow().isoformat(),\n",
    "        \"services\": {\n",
    "            \"signal_engine\": aeaas_service.signal_engine is not None,\n",
    "            \"redis\": aeaas_service.redis_client is not None,\n",
    "            \"database\": aeaas_service.db_session is not None\n",
    "        }\n",
    "    }\n",
    "\n",
    "@app.post(\"/signals/generate\", response_model=SignalResponse)\n",
    "async def generate_signals(\n",
    "    request: SignalRequestModel,\n",
    "    user_id: str = Depends(get_current_user)\n",
    "):\n",
    "    \"\"\"Generate trading signals for a symbol\"\"\"\n",
    "    # Check rate limit\n",
    "    if not await aeaas_service.check_rate_limit(user_id):\n",
    "        raise HTTPException(status_code=429, detail=\"Rate limit exceeded\")\n",
    "    \n",
    "    return await aeaas_service.generate_signals(request, user_id)\n",
    "\n",
    "@app.get(\"/signals/{request_id}\")\n",
    "async def get_signals(\n",
    "    request_id: str,\n",
    "    user_id: str = Depends(get_current_user)\n",
    "):\n",
    "    \"\"\"Get previously generated signals\"\"\"\n",
    "    # Check cache\n",
    "    if aeaas_service.redis_client:\n",
    "        cached_result = aeaas_service.redis_client.get(f\"signals:{request_id}\")\n",
    "        if cached_result:\n",
    "            return json.loads(cached_result)\n",
    "    \n",
    "    # Check database\n",
    "    db_request = aeaas_service.db_session.query(SignalRequest).filter(\n",
    "        SignalRequest.request_id == request_id,\n",
    "        SignalRequest.user_id == user_id\n",
    "    ).first()\n",
    "    \n",
    "    if not db_request:\n",
    "        raise HTTPException(status_code=404, detail=\"Signals not found\")\n",
    "    \n",
    "    return json.loads(db_request.result)\n",
    "\n",
    "@app.get(\"/user/usage\")\n",
    "async def get_user_usage(user_id: str = Depends(get_current_user)):\n",
    "    \"\"\"Get user API usage statistics\"\"\"\n",
    "    # Get current hour usage\n",
    "    current_hour = datetime.now().hour\n",
    "    hour_key = f\"rate_limit:{user_id}:{current_hour}\"\n",
    "    \n",
    "    current_usage = 0\n",
    "    if aeaas_service.redis_client:\n",
    "        current_usage = aeaas_service.redis_client.get(hour_key) or 0\n",
    "    \n",
    "    return {\n",
    "        \"user_id\": user_id,\n",
    "        \"current_hour_usage\": int(current_usage),\n",
    "        \"hourly_limit\": 1000,\n",
    "        \"remaining\": max(0, 1000 - int(current_usage))\n",
    "    }\n",
    "\n",
    "@app.websocket(\"/ws/{user_id}\")\n",
    "async def websocket_endpoint(websocket: WebSocket, user_id: str):\n",
    "    \"\"\"WebSocket endpoint for real-time signals\"\"\"\n",
    "    await aeaas_service.connection_manager.connect(websocket, user_id)\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # Wait for client message\n",
    "            data = await websocket.receive_text()\n",
    "            message = json.loads(data)\n",
    "            \n",
    "            if message.get(\"type\") == \"subscribe\":\n",
    "                # Subscribe to real-time signals\n",
    "                symbol = message.get(\"symbol\", \"BTC/USD\")\n",
    "                \n",
    "                # Send confirmation\n",
    "                await websocket.send_text(json.dumps({\n",
    "                    \"type\": \"subscribed\",\n",
    "                    \"symbol\": symbol,\n",
    "                    \"timestamp\": datetime.utcnow().isoformat()\n",
    "                }))\n",
    "                \n",
    "                # Start sending periodic updates (in production, use actual market data)\n",
    "                asyncio.create_task(\n",
    "                    send_periodic_signals(websocket, user_id, symbol)\n",
    "                )\n",
    "            \n",
    "    except WebSocketDisconnect:\n",
    "        aeaas_service.connection_manager.disconnect(websocket, user_id)\n",
    "\n",
    "async def send_periodic_signals(websocket: WebSocket, user_id: str, symbol: str):\n",
    "    \"\"\"Send periodic signal updates\"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            # Generate sample signal\n",
    "            signal_data = {\n",
    "                \"type\": \"signal\",\n",
    "                \"data\": {\n",
    "                    \"symbol\": symbol,\n",
    "                    \"timestamp\": datetime.utcnow().isoformat(),\n",
    "                    \"signal\": \"BUY\" if np.random.random() > 0.5 else \"SELL\",\n",
    "                    \"strength\": np.random.random(),\n",
    "                    \"confidence\": np.random.random()\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            await websocket.send_text(json.dumps(signal_data))\n",
    "            await asyncio.sleep(30)  # Send every 30 seconds\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"WebSocket error: {e}\")\n",
    "            break\n",
    "\n",
    "# Docker configuration\n",
    "DOCKERFILE_CONTENT = '''\n",
    "FROM python:3.11-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "COPY . .\n",
    "\n",
    "EXPOSE 8000 8001\n",
    "\n",
    "CMD [\"uvicorn\", \"aeaas:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "'''\n",
    "\n",
    "DOCKER_COMPOSE_CONTENT = '''\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  aeaas:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "      - \"8001:8001\"\n",
    "    environment:\n",
    "      - REDIS_HOST=redis\n",
    "      - DATABASE_URL=postgresql://postgres:password@db:5432/aeaas\n",
    "    depends_on:\n",
    "      - redis\n",
    "      - db\n",
    "      \n",
    "  redis:\n",
    "    image: redis:7-alpine\n",
    "    ports:\n",
    "      - \"6379:6379\"\n",
    "      \n",
    "  db:\n",
    "    image: postgres:15-alpine\n",
    "    environment:\n",
    "      - POSTGRES_DB=aeaas\n",
    "      - POSTGRES_USER=postgres\n",
    "      - POSTGRES_PASSWORD=password\n",
    "    ports:\n",
    "      - \"5432:5432\"\n",
    "    volumes:\n",
    "      - postgres_data:/var/lib/postgresql/data\n",
    "\n",
    "volumes:\n",
    "  postgres_data:\n",
    "'''\n",
    "\n",
    "# Save configuration files\n",
    "def save_docker_configs():\n",
    "    \"\"\"Save Docker configuration files\"\"\"\n",
    "    with open('/workspaces/talos/Dockerfile.aeaas', 'w') as f:\n",
    "        f.write(DOCKERFILE_CONTENT)\n",
    "    \n",
    "    with open('/workspaces/talos/docker-compose.aeaas.yml', 'w') as f:\n",
    "        f.write(DOCKER_COMPOSE_CONTENT)\n",
    "    \n",
    "    print(\"Docker configuration files saved:\")\n",
    "    print(\"- Dockerfile.aeaas\")\n",
    "    print(\"- docker-compose.aeaas.yml\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    save_docker_configs()\n",
    "    print(\"AEaaS FastAPI service configured successfully!\")\n",
    "    print(\"To run: uvicorn aeaas:app --host 0.0.0.0 --port 8000 --reload\")\n",
    "    print(\"To build Docker: docker-compose -f docker-compose.aeaas.yml up --build\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967bce99",
   "metadata": {},
   "source": [
    "## Section 5: Smart Order Routing (SOR) Simulator\n",
    "\n",
    "This section implements a sophisticated order routing simulator that:\n",
    "- Simulates multiple exchange venues with different liquidity profiles\n",
    "- Implements TWAP, VWAP, and Iceberg order execution strategies\n",
    "- Calculates optimal order splitting and routing\n",
    "- Provides market impact analysis and slippage estimation\n",
    "- Includes latency simulation and execution quality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eaee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from enum import Enum\n",
    "import asyncio\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import heapq\n",
    "import logging\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class OrderType(Enum):\n",
    "    MARKET = \"market\"\n",
    "    LIMIT = \"limit\"\n",
    "    ICEBERG = \"iceberg\"\n",
    "    TWAP = \"twap\"\n",
    "    VWAP = \"vwap\"\n",
    "    POV = \"pov\"  # Percentage of Volume\n",
    "\n",
    "class OrderSide(Enum):\n",
    "    BUY = \"buy\"\n",
    "    SELL = \"sell\"\n",
    "\n",
    "class OrderStatus(Enum):\n",
    "    PENDING = \"pending\"\n",
    "    PARTIAL = \"partial\"\n",
    "    FILLED = \"filled\"\n",
    "    CANCELLED = \"cancelled\"\n",
    "    REJECTED = \"rejected\"\n",
    "\n",
    "@dataclass\n",
    "class Exchange:\n",
    "    \"\"\"Represents a trading exchange/venue\"\"\"\n",
    "    name: str\n",
    "    latency_ms: float\n",
    "    maker_fee: float\n",
    "    taker_fee: float\n",
    "    min_size: float\n",
    "    liquidity_factor: float  # 0-1, affects available liquidity\n",
    "    market_share: float  # 0-1, market share for this venue\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.order_book = OrderBook(self.name)\n",
    "        self.executed_orders = []\n",
    "        self.total_volume = 0.0\n",
    "\n",
    "@dataclass\n",
    "class OrderBookLevel:\n",
    "    \"\"\"Single level in order book\"\"\"\n",
    "    price: float\n",
    "    size: float\n",
    "    orders: int = 1\n",
    "\n",
    "@dataclass\n",
    "class OrderBook:\n",
    "    \"\"\"Order book for an exchange\"\"\"\n",
    "    exchange_name: str\n",
    "    bids: List[OrderBookLevel] = field(default_factory=list)\n",
    "    asks: List[OrderBookLevel] = field(default_factory=list)\n",
    "    last_update: datetime = field(default_factory=datetime.now)\n",
    "    \n",
    "    def get_best_bid(self) -> Optional[OrderBookLevel]:\n",
    "        return self.bids[0] if self.bids else None\n",
    "    \n",
    "    def get_best_ask(self) -> Optional[OrderBookLevel]:\n",
    "        return self.asks[0] if self.asks else None\n",
    "    \n",
    "    def get_spread(self) -> float:\n",
    "        bid = self.get_best_bid()\n",
    "        ask = self.get_best_ask()\n",
    "        if bid and ask:\n",
    "            return ask.price - bid.price\n",
    "        return 0.0\n",
    "    \n",
    "    def get_mid_price(self) -> float:\n",
    "        bid = self.get_best_bid()\n",
    "        ask = self.get_best_ask()\n",
    "        if bid and ask:\n",
    "            return (bid.price + ask.price) / 2\n",
    "        return 0.0\n",
    "\n",
    "@dataclass\n",
    "class Order:\n",
    "    \"\"\"Trading order\"\"\"\n",
    "    order_id: str\n",
    "    symbol: str\n",
    "    side: OrderSide\n",
    "    order_type: OrderType\n",
    "    size: float\n",
    "    price: Optional[float] = None\n",
    "    \n",
    "    # Execution parameters\n",
    "    max_participation: float = 0.1  # Max % of volume\n",
    "    time_in_force: str = \"DAY\"\n",
    "    display_size: Optional[float] = None  # For iceberg orders\n",
    "    \n",
    "    # Status tracking\n",
    "    status: OrderStatus = OrderStatus.PENDING\n",
    "    filled_size: float = 0.0\n",
    "    remaining_size: float = 0.0\n",
    "    avg_fill_price: float = 0.0\n",
    "    \n",
    "    # Routing\n",
    "    target_exchanges: List[str] = field(default_factory=list)\n",
    "    child_orders: List['Order'] = field(default_factory=list)\n",
    "    \n",
    "    # Timestamps\n",
    "    created_at: datetime = field(default_factory=datetime.now)\n",
    "    updated_at: datetime = field(default_factory=datetime.now)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.remaining_size = self.size\n",
    "\n",
    "@dataclass\n",
    "class Fill:\n",
    "    \"\"\"Order fill/execution\"\"\"\n",
    "    fill_id: str\n",
    "    order_id: str\n",
    "    exchange: str\n",
    "    size: float\n",
    "    price: float\n",
    "    fee: float\n",
    "    timestamp: datetime\n",
    "    side: OrderSide\n",
    "    \n",
    "class MarketDataSimulator:\n",
    "    \"\"\"Simulates realistic market data for multiple exchanges\"\"\"\n",
    "    \n",
    "    def __init__(self, symbol: str, base_price: float = 50000.0):\n",
    "        self.symbol = symbol\n",
    "        self.base_price = base_price\n",
    "        self.current_price = base_price\n",
    "        self.volatility = 0.02\n",
    "        self.trend = 0.0\n",
    "        \n",
    "    def generate_order_book(self, exchange: Exchange, depth: int = 10) -> OrderBook:\n",
    "        \"\"\"Generate realistic order book for an exchange\"\"\"\n",
    "        # Adjust liquidity based on exchange characteristics\n",
    "        liquidity_multiplier = exchange.liquidity_factor\n",
    "        spread_multiplier = 1.0 / exchange.liquidity_factor\n",
    "        \n",
    "        # Calculate base spread\n",
    "        base_spread = self.current_price * 0.001 * spread_multiplier\n",
    "        \n",
    "        order_book = OrderBook(exchange.name)\n",
    "        \n",
    "        # Generate bids\n",
    "        bid_price = self.current_price - base_spread / 2\n",
    "        for i in range(depth):\n",
    "            level_spread = base_spread * (1 + i * 0.5)\n",
    "            price = self.current_price - level_spread\n",
    "            \n",
    "            # Size decreases with distance from mid\n",
    "            base_size = 100 * liquidity_multiplier * (1 - i * 0.1)\n",
    "            size = base_size * (1 + np.random.normal(0, 0.2))\n",
    "            \n",
    "            order_book.bids.append(OrderBookLevel(\n",
    "                price=price,\n",
    "                size=max(0.1, size),\n",
    "                orders=random.randint(1, 5)\n",
    "            ))\n",
    "        \n",
    "        # Generate asks\n",
    "        for i in range(depth):\n",
    "            level_spread = base_spread * (1 + i * 0.5)\n",
    "            price = self.current_price + level_spread\n",
    "            \n",
    "            base_size = 100 * liquidity_multiplier * (1 - i * 0.1)\n",
    "            size = base_size * (1 + np.random.normal(0, 0.2))\n",
    "            \n",
    "            order_book.asks.append(OrderBookLevel(\n",
    "                price=price,\n",
    "                size=max(0.1, size),\n",
    "                orders=random.randint(1, 5)\n",
    "            ))\n",
    "        \n",
    "        # Sort order book\n",
    "        order_book.bids.sort(key=lambda x: x.price, reverse=True)\n",
    "        order_book.asks.sort(key=lambda x: x.price)\n",
    "        \n",
    "        return order_book\n",
    "    \n",
    "    def update_price(self, dt: float = 1.0):\n",
    "        \"\"\"Update price using random walk with trend\"\"\"\n",
    "        drift = self.trend * dt\n",
    "        diffusion = self.volatility * np.sqrt(dt) * np.random.normal()\n",
    "        \n",
    "        self.current_price *= (1 + drift + diffusion)\n",
    "        self.current_price = max(self.current_price, 0.01)  # Prevent negative prices\n",
    "\n",
    "class SmartOrderRouter:\n",
    "    \"\"\"Smart Order Routing engine\"\"\"\n",
    "    \n",
    "    def __init__(self, exchanges: List[Exchange], market_data: MarketDataSimulator):\n",
    "        self.exchanges = {ex.name: ex for ex in exchanges}\n",
    "        self.market_data = market_data\n",
    "        self.active_orders: Dict[str, Order] = {}\n",
    "        self.execution_queue = []\n",
    "        self.fills: List[Fill] = []\n",
    "        \n",
    "        # Performance metrics\n",
    "        self.metrics = {\n",
    "            'total_volume': 0.0,\n",
    "            'total_fees': 0.0,\n",
    "            'avg_slippage': 0.0,\n",
    "            'avg_latency': 0.0,\n",
    "            'fill_rate': 0.0\n",
    "        }\n",
    "        \n",
    "    def analyze_market_conditions(self) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze current market conditions across all exchanges\"\"\"\n",
    "        conditions = {\n",
    "            'exchange_data': {},\n",
    "            'best_bid': 0.0,\n",
    "            'best_ask': float('inf'),\n",
    "            'total_bid_liquidity': 0.0,\n",
    "            'total_ask_liquidity': 0.0,\n",
    "            'weighted_mid_price': 0.0\n",
    "        }\n",
    "        \n",
    "        total_weight = 0.0\n",
    "        weighted_price_sum = 0.0\n",
    "        \n",
    "        for exchange in self.exchanges.values():\n",
    "            # Generate fresh order book\n",
    "            order_book = self.market_data.generate_order_book(exchange)\n",
    "            exchange.order_book = order_book\n",
    "            \n",
    "            best_bid = order_book.get_best_bid()\n",
    "            best_ask = order_book.get_best_ask()\n",
    "            \n",
    "            exchange_data = {\n",
    "                'best_bid': best_bid.price if best_bid else 0.0,\n",
    "                'best_ask': best_ask.price if best_ask else float('inf'),\n",
    "                'bid_size': best_bid.size if best_bid else 0.0,\n",
    "                'ask_size': best_ask.size if best_ask else 0.0,\n",
    "                'spread': order_book.get_spread(),\n",
    "                'mid_price': order_book.get_mid_price(),\n",
    "                'liquidity_score': exchange.liquidity_factor\n",
    "            }\n",
    "            \n",
    "            conditions['exchange_data'][exchange.name] = exchange_data\n",
    "            \n",
    "            # Update best prices\n",
    "            if best_bid and best_bid.price > conditions['best_bid']:\n",
    "                conditions['best_bid'] = best_bid.price\n",
    "            if best_ask and best_ask.price < conditions['best_ask']:\n",
    "                conditions['best_ask'] = best_ask.price\n",
    "            \n",
    "            # Calculate weighted mid price\n",
    "            if best_bid and best_ask:\n",
    "                weight = exchange.market_share\n",
    "                mid_price = (best_bid.price + best_ask.price) / 2\n",
    "                weighted_price_sum += mid_price * weight\n",
    "                total_weight += weight\n",
    "        \n",
    "        if total_weight > 0:\n",
    "            conditions['weighted_mid_price'] = weighted_price_sum / total_weight\n",
    "        \n",
    "        return conditions\n",
    "    \n",
    "    def calculate_market_impact(self, order: Order, exchange: Exchange) -> float:\n",
    "        \"\"\"Calculate expected market impact for an order\"\"\"\n",
    "        order_book = exchange.order_book\n",
    "        \n",
    "        if order.side == OrderSide.BUY:\n",
    "            levels = order_book.asks\n",
    "        else:\n",
    "            levels = order_book.bids\n",
    "        \n",
    "        if not levels:\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate impact based on order size relative to available liquidity\n",
    "        remaining_size = order.size\n",
    "        total_cost = 0.0\n",
    "        \n",
    "        for level in levels:\n",
    "            if remaining_size <= 0:\n",
    "                break\n",
    "            \n",
    "            size_to_take = min(remaining_size, level.size)\n",
    "            total_cost += size_to_take * level.price\n",
    "            remaining_size -= size_to_take\n",
    "        \n",
    "        if remaining_size > 0:\n",
    "            # Order larger than available liquidity\n",
    "            return 0.1  # 10% impact for large orders\n",
    "        \n",
    "        avg_price = total_cost / order.size\n",
    "        reference_price = self.market_data.current_price\n",
    "        \n",
    "        return abs(avg_price - reference_price) / reference_price\n",
    "    \n",
    "    def optimize_order_routing(self, order: Order) -> List[Tuple[str, float]]:\n",
    "        \\\"\\\"\\\"Optimize order routing across exchanges\\\"\\\"\\\"\\n        conditions = self.analyze_market_conditions()\\n        \\n        # Calculate scores for each exchange\\n        exchange_scores = {}\\n        \\n        for exchange_name, exchange in self.exchanges.items():\\n            exchange_data = conditions['exchange_data'][exchange_name]\\n            \\n            # Skip if no liquidity\\n            if order.side == OrderSide.BUY and exchange_data['ask_size'] == 0:\\n                continue\\n            if order.side == OrderSide.SELL and exchange_data['bid_size'] == 0:\\n                continue\\n            \\n            # Calculate score based on multiple factors\\n            price_score = self._calculate_price_score(order, exchange_data)\\n            liquidity_score = exchange.liquidity_factor\\n            fee_score = 1.0 - (exchange.taker_fee + exchange.maker_fee)\\n            latency_score = 1.0 - (exchange.latency_ms / 1000.0)\\n            \\n            # Market impact penalty\\n            impact = self.calculate_market_impact(order, exchange)\\n            impact_score = 1.0 - impact\\n            \\n            # Weighted combination\\n            total_score = (\\n                price_score * 0.3 +\\n                liquidity_score * 0.25 +\\n                fee_score * 0.2 +\\n                latency_score * 0.1 +\\n                impact_score * 0.15\\n            )\\n            \\n            exchange_scores[exchange_name] = {\\n                'total_score': total_score,\\n                'available_size': exchange_data['bid_size'] if order.side == OrderSide.SELL else exchange_data['ask_size']\\n            }\\n        \\n        # Sort by score\\n        sorted_exchanges = sorted(\\n            exchange_scores.items(),\\n            key=lambda x: x[1]['total_score'],\\n            reverse=True\\n        )\\n        \\n        # Allocate order size across exchanges\\n        allocations = []\\n        remaining_size = order.size\\n        \\n        for exchange_name, score_data in sorted_exchanges:\\n            if remaining_size <= 0:\\n                break\\n            \\n            # Allocate based on available liquidity and score\\n            max_allocation = min(remaining_size, score_data['available_size'])\\n            \\n            # Don't allocate more than 50% to any single exchange for large orders\\n            if order.size > 1000:\\n                max_allocation = min(max_allocation, order.size * 0.5)\\n            \\n            if max_allocation > 0:\\n                allocations.append((exchange_name, max_allocation))\\n                remaining_size -= max_allocation\\n        \\n        # If we couldn't allocate everything, scale up proportionally\\n        if remaining_size > 0 and allocations:\\n            scale_factor = order.size / (order.size - remaining_size)\\n            allocations = [(ex, size * scale_factor) for ex, size in allocations]\\n        \\n        return allocations\\n    \\n    def _calculate_price_score(self, order: Order, exchange_data: Dict) -> float:\\n        \\\"\\\"\\\"Calculate price score for an exchange\\\"\\\"\\\"\\n        if order.side == OrderSide.BUY:\\n            price = exchange_data['best_ask']\\n            # Lower ask price is better for buy orders\\n            return 1.0 - (price - conditions['best_ask']) / conditions['best_ask']\\n        else:\\n            price = exchange_data['best_bid']\\n            # Higher bid price is better for sell orders\\n            return 1.0 - (conditions['best_bid'] - price) / conditions['best_bid']\\n    \\n    async def execute_order(self, order: Order) -> List[Fill]:\\n        \\\"\\\"\\\"Execute order using smart routing\\\"\\\"\\\"\\n        self.active_orders[order.order_id] = order\\n        \\n        if order.order_type == OrderType.MARKET:\\n            return await self._execute_market_order(order)\\n        elif order.order_type == OrderType.TWAP:\\n            return await self._execute_twap_order(order)\\n        elif order.order_type == OrderType.VWAP:\\n            return await self._execute_vwap_order(order)\\n        elif order.order_type == OrderType.ICEBERG:\\n            return await self._execute_iceberg_order(order)\\n        else:\\n            return await self._execute_limit_order(order)\\n    \\n    async def _execute_market_order(self, order: Order) -> List[Fill]:\\n        \\\"\\\"\\\"Execute market order with smart routing\\\"\\\"\\\"\\n        allocations = self.optimize_order_routing(order)\\n        fills = []\\n        \\n        for exchange_name, size in allocations:\\n            exchange = self.exchanges[exchange_name]\\n            \\n            # Simulate execution latency\\n            await asyncio.sleep(exchange.latency_ms / 1000.0)\\n            \\n            # Get current order book\\n            order_book = exchange.order_book\\n            \\n            if order.side == OrderSide.BUY:\\n                levels = order_book.asks\\n            else:\\n                levels = order_book.bids\\n            \\n            # Execute against available liquidity\\n            remaining_size = size\\n            for level in levels:\\n                if remaining_size <= 0:\\n                    break\\n                \\n                fill_size = min(remaining_size, level.size)\\n                fill_price = level.price\\n                \\n                # Calculate fees\\n                fee = fill_size * fill_price * exchange.taker_fee\\n                \\n                # Create fill\\n                fill = Fill(\\n                    fill_id=f\\\"fill_{len(self.fills) + 1}\\\",\\n                    order_id=order.order_id,\\n                    exchange=exchange_name,\\n                    size=fill_size,\\n                    price=fill_price,\\n                    fee=fee,\\n                    timestamp=datetime.now(),\\n                    side=order.side\\n                )\\n                \\n                fills.append(fill)\\n                self.fills.append(fill)\\n                \\n                # Update order\\n                order.filled_size += fill_size\\n                order.remaining_size -= fill_size\\n                \\n                # Update level liquidity\\n                level.size -= fill_size\\n                \\n                remaining_size -= fill_size\\n        \\n        # Update order status\\n        if order.remaining_size == 0:\\n            order.status = OrderStatus.FILLED\\n        elif order.filled_size > 0:\\n            order.status = OrderStatus.PARTIAL\\n        \\n        # Calculate average fill price\\n        if order.filled_size > 0:\\n            total_cost = sum(fill.size * fill.price for fill in fills)\\n            order.avg_fill_price = total_cost / order.filled_size\\n        \\n        return fills\\n    \\n    async def _execute_twap_order(self, order: Order, duration_minutes: int = 60) -> List[Fill]:\\n        \\\"\\\"\\\"Execute TWAP (Time-Weighted Average Price) order\\\"\\\"\\\"\\n        all_fills = []\\n        \\n        # Split order into time slices\\n        num_slices = max(1, duration_minutes // 5)  # 5-minute slices\\n        slice_size = order.size / num_slices\\n        \\n        for i in range(num_slices):\\n            if order.remaining_size <= 0:\\n                break\\n            \\n            # Create child order for this slice\\n            child_order = Order(\\n                order_id=f\\\"{order.order_id}_slice_{i}\\\",\\n                symbol=order.symbol,\\n                side=order.side,\\n                order_type=OrderType.MARKET,\\n                size=min(slice_size, order.remaining_size)\\n            )\\n            \\n            # Execute child order\\n            fills = await self._execute_market_order(child_order)\\n            all_fills.extend(fills)\\n            \\n            # Update parent order\\n            order.filled_size += child_order.filled_size\\n            order.remaining_size -= child_order.filled_size\\n            \\n            # Wait before next slice (simulate time passage)\\n            if i < num_slices - 1:\\n                await asyncio.sleep(1)  # Simulate 5-minute wait\\n        \\n        # Update order status\\n        if order.remaining_size == 0:\\n            order.status = OrderStatus.FILLED\\n        elif order.filled_size > 0:\\n            order.status = OrderStatus.PARTIAL\\n        \\n        return all_fills\\n    \\n    async def _execute_vwap_order(self, order: Order) -> List[Fill]:\\n        \\\"\\\"\\\"Execute VWAP (Volume-Weighted Average Price) order\\\"\\\"\\\"\\n        # For simulation, treat similar to TWAP but with volume-based timing\\n        return await self._execute_twap_order(order, duration_minutes=30)\\n    \\n    async def _execute_iceberg_order(self, order: Order) -> List[Fill]:\\n        \\\"\\\"\\\"Execute iceberg order (hidden size)\\\"\\\"\\\"\\n        all_fills = []\\n        display_size = order.display_size or (order.size * 0.1)  # Show 10% by default\\n        \\n        while order.remaining_size > 0:\\n            # Create visible child order\\n            child_size = min(display_size, order.remaining_size)\\n            child_order = Order(\\n                order_id=f\\\"{order.order_id}_iceberg_{len(all_fills)}\\\",\\n                symbol=order.symbol,\\n                side=order.side,\\n                order_type=OrderType.MARKET,\\n                size=child_size\\n            )\\n            \\n            # Execute child order\\n            fills = await self._execute_market_order(child_order)\\n            all_fills.extend(fills)\\n            \\n            # Update parent order\\n            order.filled_size += child_order.filled_size\\n            order.remaining_size -= child_order.filled_size\\n            \\n            # Small delay before showing next iceberg slice\\n            await asyncio.sleep(0.1)\\n        \\n        order.status = OrderStatus.FILLED\\n        return all_fills\\n    \\n    async def _execute_limit_order(self, order: Order) -> List[Fill]:\\n        \\\"\\\"\\\"Execute limit order (simplified)\\\"\\\"\\\"\\n        # For simulation, convert to market order if price is favorable\\n        conditions = self.analyze_market_conditions()\\n        \\n        if order.side == OrderSide.BUY and order.price >= conditions['best_ask']:\\n            return await self._execute_market_order(order)\\n        elif order.side == OrderSide.SELL and order.price <= conditions['best_bid']:\\n            return await self._execute_market_order(order)\\n        else:\\n            # Order not immediately executable\\n            order.status = OrderStatus.PENDING\\n            return []\\n    \\n    def generate_execution_report(self, order: Order) -> Dict[str, Any]:\\n        \\\"\\\"\\\"Generate execution quality report\\\"\\\"\\\"\\n        order_fills = [fill for fill in self.fills if fill.order_id == order.order_id]\\n        \\n        if not order_fills:\\n            return {'error': 'No fills found for order'}\\n        \\n        # Calculate metrics\\n        total_size = sum(fill.size for fill in order_fills)\\n        total_cost = sum(fill.size * fill.price for fill in order_fills)\\n        total_fees = sum(fill.fee for fill in order_fills)\\n        \\n        avg_price = total_cost / total_size if total_size > 0 else 0\\n        \\n        # Calculate slippage vs. market price at order time\\n        market_price = self.market_data.current_price\\n        slippage = abs(avg_price - market_price) / market_price\\n        \\n        # Exchange breakdown\\n        exchange_breakdown = {}\\n        for fill in order_fills:\\n            if fill.exchange not in exchange_breakdown:\\n                exchange_breakdown[fill.exchange] = {\\n                    'size': 0,\\n                    'cost': 0,\\n                    'fees': 0,\\n                    'fills': 0\\n                }\\n            \\n            exchange_breakdown[fill.exchange]['size'] += fill.size\\n            exchange_breakdown[fill.exchange]['cost'] += fill.size * fill.price\\n            exchange_breakdown[fill.exchange]['fees'] += fill.fee\\n            exchange_breakdown[fill.exchange]['fills'] += 1\\n        \\n        return {\\n            'order_id': order.order_id,\\n            'total_size': total_size,\\n            'filled_size': order.filled_size,\\n            'fill_rate': order.filled_size / order.size,\\n            'avg_price': avg_price,\\n            'total_cost': total_cost,\\n            'total_fees': total_fees,\\n            'slippage': slippage,\\n            'num_fills': len(order_fills),\\n            'exchange_breakdown': exchange_breakdown,\\n            'execution_time': (order.updated_at - order.created_at).total_seconds()\\n        }\\n\\n# Example usage and testing\\nif __name__ == \\\"__main__\\\":\\n    # Create exchanges\\n    exchanges = [\\n        Exchange(\\\"Binance\\\", latency_ms=50, maker_fee=0.001, taker_fee=0.001, \\n                min_size=0.001, liquidity_factor=0.9, market_share=0.4),\\n        Exchange(\\\"Coinbase\\\", latency_ms=100, maker_fee=0.005, taker_fee=0.005, \\n                min_size=0.001, liquidity_factor=0.8, market_share=0.3),\\n        Exchange(\\\"Kraken\\\", latency_ms=80, maker_fee=0.0016, taker_fee=0.0026, \\n                min_size=0.001, liquidity_factor=0.7, market_share=0.2),\\n        Exchange(\\\"FTX\\\", latency_ms=30, maker_fee=0.0002, taker_fee=0.0007, \\n                min_size=0.001, liquidity_factor=0.6, market_share=0.1)\\n    ]\\n    \\n    # Create market data simulator\\n    market_sim = MarketDataSimulator(\\\"BTC/USD\\\", base_price=50000.0)\\n    \\n    # Create smart order router\\n    router = SmartOrderRouter(exchanges, market_sim)\\n    \\n    # Example orders\\n    orders = [\\n        Order(\\\"order_1\\\", \\\"BTC/USD\\\", OrderSide.BUY, OrderType.MARKET, 5.0),\\n        Order(\\\"order_2\\\", \\\"BTC/USD\\\", OrderSide.SELL, OrderType.TWAP, 10.0),\\n        Order(\\\"order_3\\\", \\\"BTC/USD\\\", OrderSide.BUY, OrderType.ICEBERG, 15.0, display_size=2.0)\\n    ]\\n    \\n    async def run_simulation():\\n        print(\\\"Starting Smart Order Routing Simulation...\\\")\\n        \\n        for order in orders:\\n            print(f\\\"\\\\nExecuting {order.order_type.value} order: {order.size} {order.symbol}\\\")\\n            \\n            # Execute order\\n            fills = await router.execute_order(order)\\n            \\n            # Generate report\\n            report = router.generate_execution_report(order)\\n            \\n            print(f\\\"Order {order.order_id} Results:\\\")\\n            print(f\\\"  Fill Rate: {report['fill_rate']:.2%}\\\")\\n            print(f\\\"  Avg Price: ${report['avg_price']:.2f}\\\")\\n            print(f\\\"  Slippage: {report['slippage']:.4%}\\\")\\n            print(f\\\"  Total Fees: ${report['total_fees']:.2f}\\\")\\n            print(f\\\"  Exchanges Used: {list(report['exchange_breakdown'].keys())}\\\")\\n    \\n    # Run simulation\\n    import asyncio\\n    asyncio.run(run_simulation())\\n    \\n    print(\\\"\\\\nSmart Order Routing Simulator completed successfully!\\\")\"\n",
    "    ]\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
